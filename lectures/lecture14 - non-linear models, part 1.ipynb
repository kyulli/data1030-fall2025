{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mudcard\n",
    "- **In which situation we should use RMSE over MSE?**\n",
    "    - Either works\n",
    "    - I have a slight preference for RMSE over MSE because RMSE has the same unit as the target variable\n",
    "    - Non-technical audiences have an easier time understanding RMSE in my experience\n",
    "- **When to use ROC vs PRC**\n",
    "    - Use the precision recall curve if your classification problem is imbalanced\n",
    "- **Should we memorize these equations too? I am still trying to understand these metrics but the equations/maths = muddy**\n",
    "    - Yes :)\n",
    "    - The idea is that once you understand these metrics, you will remember the equations because they will make sense, so you won't just simply memorize them.\n",
    "- **what should we take care when apply The logloss metric**\n",
    "    - I'm not sure I understand the question.\n",
    "    - Could you please expand on this and post on the course forum?\n",
    "- **just wanted to clarify what it means to have a bad classifier, either in the continuous or categorical case? would good and bad classifiers be symmetric in the sense that if you classify all points wrong you classify all points right, just with some kind of inversion applied?**\n",
    "    - classification has a categorical target variable\n",
    "    - regression has a continuous target variable\n",
    "    - No, if a classifier gives worse than baseline predictions, it is not a good idea to invert the predictions\n",
    "    - worse than baseline classifiers usually mean there is a bug in your code\n",
    "    - focus your effort on finding and fixing that bug\n",
    "    - you never want to deploy a buggy model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of what we covered so far and what's coming\n",
    "\n",
    "Complications in the ML pipeline:\n",
    "\n",
    "- if your dataset is non-iid, address it in splitting\n",
    "    - time series data: use VAR(p), and sklearn's TimeSeriesSplit\n",
    "    - group structure: use the group based slitters\n",
    "        - if there are categorical features in your dataset, it does not mean that there is group structure!\n",
    "        - look for ID columns instead\n",
    "- if a classification problem is imbalanced, address it in splitting\n",
    "    - use stratified splitting methods \n",
    "- if your dataset has missing values in categorical and ordinal features, address it in preprocessing\n",
    "    - treat missing values as another category\n",
    "    - do not impute, do not drop columns\n",
    "- if dataset is imbalanced, use evaluation metrics that do not rely on the True Negative element of the confusion matrix\n",
    "    - no accuracy, no ROC curve for example\n",
    "    - use precision, recall, f score, p-r curve, etc instead\n",
    "- if your dataset is imbalanced, be careful about oversampling and undersampling methods - Lecture 17\n",
    "- missing values in continuous features: Lecture 18-19\n",
    "- interpretability: Lecture 20-21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The supervised ML pipeline\n",
    "\n",
    "**0. Data collection/manipulation**: you might have multiple data sources and/or you might have more data than you need\n",
    "   - you need to be able to read in datasets from various sources (like csv, excel, SQL, parquet, etc)\n",
    "   - you need to be able to filter the columns/rows you need for your ML model\n",
    "   - you need to be able to combine the datasets into one dataframe \n",
    "\n",
    "**1. Exploratory Data Analysis (EDA)**: you need to understand your data and verify that it doesn't contain errors\n",
    "   - do as much EDA as you can!\n",
    "    \n",
    "**2. Split the data into different sets**: most often the sets are train, validation, and test (or holdout)\n",
    "   - practitioners often make errors in this step!\n",
    "   - you can split the data randomly, based on groups, based on time, or any other non-standard way if necessary to answer your ML question\n",
    "\n",
    "**3. Preprocess the data**: ML models only work if X and Y are numbers! Some ML models additionally require each feature to have 0 mean and 1 standard deviation (standardized features)\n",
    "   - often the original features you get contain strings (for example a gender feature would contain 'male', 'female', 'non-binary', 'unknown') which needs to be transformed into numbers\n",
    "   - often the features are not standardized (e.g., age is between 0 and 100) but it needs to be standardized\n",
    "    \n",
    "**4. Choose an evaluation metric**: depends on the priorities of the stakeholders\n",
    "   - often requires quite a bit of thinking and ethical considerations\n",
    "     \n",
    "<span style=\"background-color: #FFFF00\">**5. Choose one or more ML techniques**: it is highly recommended that you try multiple models</span>\n",
    "   - start with simple models like linear or logistic regression\n",
    "   - try also more complex models like nearest neighbors, support vector machines, random forest, etc.\n",
    "    \n",
    "**6. Tune the hyperparameters of your ML models (aka cross-validation or hyperparameter tuning)**\n",
    "   - ML techniques have hyperparameters that you need to optimize to achieve best performance\n",
    "   - for each ML model, decide which parameters to tune and what values to try\n",
    "   - loop through each parameter combination\n",
    "       - train one model for each parameter combination\n",
    "       - evaluate how well the model performs on the validation set\n",
    "   - take the parameter combo that gives the best validation score\n",
    "   - evaluate that model on the test set to report how well the model is expected to perform on previously unseen data\n",
    "    \n",
    "**7. Interpret your model**: black boxes are often not useful\n",
    "   - check if your model uses features that make sense (excellent tool for debugging)\n",
    "   - often model predictions are not enough, you need to be able to explain how the model arrived to a particular prediction (e.g., in health care)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Non-linear ML algorithms\n",
    "By the end of this lecture, you will be able to\n",
    "- Summarize the no free lunch theorem\n",
    "- Describe how decision trees work\n",
    "- Describe how a random forest works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='LIGHTGRAY'>Non-linear ML algorithms</font>\n",
    "<font color='LIGHTGRAY'>By the end of this lecture, you will be able to</font>\n",
    "- **Summarize the no free lunch theorem**\n",
    "- <font color='LIGHTGRAY'>Describe how decision trees work</font>\n",
    "- <font color='LIGHTGRAY'>Describe how a random forest works</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Which ML algorithm to try on your dataset?\n",
    "\n",
    "- **No supervised ML algorithm is universally better than all others** - no free lunch theorem\n",
    "- **For every learning algorithm, there is a dataset/task on which it fails. Even though another algorithm can successfully learn on it.**\n",
    "- If we had a universally good algorithm, we all just would use that one algorithm.\n",
    "- you need to try as many as you can to find the one that performs best\n",
    "- you will see during the final project presentations that different models perform best for different datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='LIGHTGRAY'>Non-linear ML algorithms</font>\n",
    "<font color='LIGHTGRAY'>By the end of this lecture, you will be able to</font>\n",
    "- <font color='LIGHTGRAY'>Summarize the no free lunch theorem</font>\n",
    "- **Describe how decision trees work**\n",
    "- <font color='LIGHTGRAY'>Describe how a random forest works</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Decision trees</center>\n",
    "\n",
    "- no free lunch theorem in action!\n",
    "- deep learning tools are all the rage nowadays but tree-based methods outperform deep learning tools on tabular data\n",
    "- see NeurIPS paper [here](https://papers.neurips.cc/paper_files/paper/2022/file/0378c7692da36807bdec87ab043cdadc-Paper-Datasets_and_Benchmarks.pdf)\n",
    "- we discuss decision trees and random forest today, gradient boosting and XGBoost on Monday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Decision tree: the data is split according to certain features\n",
    "- Here is an example tree fitted to data:\n",
    "\n",
    "<center><img src = '../figures/Decision-Trees-modified-1.png' width=\"500\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Trees have nodes and leaves.\n",
    "    - Nodes are where the split happens\n",
    "    - Leaves are where we predict the target variable\n",
    "- The depth of the decision tree is the number of nodes along the longest path from the root node to a leaf.\n",
    "    - The tree above has a depth of 2.\n",
    "- Notice that each split is an if-else statement (binary split), and only one feature is split on each node\n",
    "    - splits with three or more branches or complex conditions (i.e., split over multiple features in one node) are not done.\n",
    "- The critical values and features in the nodes are determined automatically by minimizing a cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='LIGHTGRAY'>Non-linear ML algorithms</font>\n",
    "<font color='LIGHTGRAY'>By the end of this lecture, you will be able to</font>\n",
    "- <font color='LIGHTGRAY'>Summarize the no free lunch theorem</font>\n",
    "- <font color='LIGHTGRAY'>Describe how decision trees work</font>\n",
    "- **Describe how a random forest works**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Random forest\n",
    "\n",
    "- Random forest: ensemble of independent random decision trees\n",
    "- Each tree sees a random subset of the training data, that's why the forest is random.\n",
    "    - each tree is trained on a random subset of features or a random subset of points, or both\n",
    "- Majority voting among the trees determines the final predicted class\n",
    "- the fraction of votes is used to calculate predicted probabilities\n",
    "\n",
    "<center><img src=\"../figures/tree.png\" width=\"450\"></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Quiz 2\n",
    "- **Use the dataset below and create a decision tree with max_depth = 2 to predict the target variable!**\n",
    "- **What is your tree's prediction for each person?**\n",
    "- Remember, you might not be able to find a tree that predicts all points perfectly.\n",
    "- It just needs to get as many points as possible right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use columns with 0-based indices [4 2 2] to create your decision tree.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# which features should your team use?\n",
    "ftrs_to_use = np.random.randint(0,6,size=3)\n",
    "print('use columns with 0-based indices', ftrs_to_use, 'to create your decision tree.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "| X|age|gender (M=0, F=1)|is student?|is parent?|uses computer for work?|nr. of hours on c.|<font color='red'>Like computer games?</font>|\n",
    "|-|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|__person 0__| 5|0|1|0|0|0.0|__<font color='red'>1</font>__|\n",
    "|__person 1__|48|1|0|1|0|1.8|__<font color='red'>1</font>__|\n",
    "|__person 2__|62|0|0|1|0|0.2|__<font color='red'>0</font>__|\n",
    "|__person 3__|10|1|1|0|0|2.4|__<font color='red'>1</font>__|\n",
    "|__person 4__|23|1|1|0|1|4.2|__<font color='red'>0</font>__|\n",
    "|__person 5__|36|0|0|0|1|3.1|__<font color='red'>1</font>__|\n",
    "|__person 6__|12|0|1|0|0|3.1|__<font color='red'>1</font>__|\n",
    "|__person 7__|85|0|0|0|1|1.0|__<font color='red'>0</font>__|\n",
    "|__person 8__|33|1|1|1|0|1.5|__<font color='red'>0</font>__|\n",
    "|__person 9__|56|0|0|0|1|0.1|__<font color='red'>1</font>__|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mud card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
